{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "external_retrieved_words_microtext.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UV8lt9xVaK5",
        "outputId": "9151eeed-dd60-43d7-9787-1f1868e680ee"
      },
      "source": [
        "!pip install mendelai-brat-parser\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mendelai-brat-parser\n",
            "  Downloading mendelai_brat_parser-0.0.4-py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: mendelai-brat-parser\n",
            "Successfully installed mendelai-brat-parser-0.0.4\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BklOuMfjrMpZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "import regex as re\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQd31KC5YZMb"
      },
      "source": [
        "# Imported words and bigrams indicated in the appendix of the guidelines of \n",
        "# the Stab and Gurevych dataset as indicative of a premise or of a claim.\n",
        "# The n-gram which are absent in each one of the dataset considered have been\n",
        "# already removed from the list.\n",
        "\n",
        "claim_premise_indicators=['accordingly','consequently','conclude that','clearly','demonstrates that','hence','implies','in short','in conclusion','indicates that','it follows that',\n",
        "'it should be clear that','it should be clear','so','suggests that','therefore','thus','to sum up','assuming that','as',\n",
        "'besides','because','deduced','derived from','due to','for','for example','for instance','for the reason that','furthermore','given that','in addition','in light of','in that','in view of','indicated by','is supported by','moreover','since',\n",
        "'whereas']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3aIwBjCGLvi"
      },
      "source": [
        "# **Dataset import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "uvZ5p0jwmRRi",
        "outputId": "b3b749b8-0209-4ed3-c80f-659317c69426"
      },
      "source": [
        "# Import the microtexts dataset.\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17871749-da5d-4cf5-ab23-42d52785c698\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-17871749-da5d-4cf5-ab23-42d52785c698\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving arg-microtexts-master.zip to arg-microtexts-master.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq9YeH2LmrOs"
      },
      "source": [
        "# Function to extract zip file.\n",
        "# Takes in input the path to the zip file (path_zip) and the one to store the destination directory (path_destination).\n",
        "\n",
        "def extract_zip(path_zip,path_destination):\n",
        "  with zipfile.ZipFile(path_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path_destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqHyoIBamyF3"
      },
      "source": [
        "# Extract zip file.\n",
        "\n",
        "extract_zip('arg-microtexts-master.zip','arg-microtexts-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrsWTewaCnPf"
      },
      "source": [
        "# **Extract argumentative sections and their relations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DevYxwAnSM5"
      },
      "source": [
        "# Extract the list of xml files containing the argumentative sections in the \n",
        "# dataset and their relations.\n",
        "\n",
        "xml_files = sorted(glob.glob(\"arg-microtexts-master/arg-microtexts-master/corpus/en/micro_*.xml\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtLsSo_ToNmb",
        "outputId": "f0b084d5-defa-4052-d3b4-393b0fab21f6"
      },
      "source": [
        "# List of xml files of the essays.\n",
        "xml_files[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arg-microtexts-master/arg-microtexts-master/corpus/en/micro_b001.xml',\n",
              " 'arg-microtexts-master/arg-microtexts-master/corpus/en/micro_b002.xml',\n",
              " 'arg-microtexts-master/arg-microtexts-master/corpus/en/micro_b003.xml',\n",
              " 'arg-microtexts-master/arg-microtexts-master/corpus/en/micro_b004.xml',\n",
              " 'arg-microtexts-master/arg-microtexts-master/corpus/en/micro_b005.xml']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzUf3TJ7ogEx"
      },
      "source": [
        "# Transform each xml file into the xml tree representation.\n",
        "\n",
        "list_xml_tree_representation=[ET.parse(xml_file) for xml_file in xml_files]\n",
        "\n",
        "# Than extract from each tree its root.\n",
        "\n",
        "list_xml_root=[tree.getroot() for tree in list_xml_tree_representation]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3f-td1Epkrf"
      },
      "source": [
        "# list_argumentative_sections will contain the lists, for each document in the\n",
        "# corpus, of its argumentative sections.\n",
        "list_argumentative_sections=[]\n",
        "# list_arg_section_id_in_document will contain the lists, for each document in\n",
        "# the corpus, of the id that identify the sections inside the document.\n",
        "list_arg_section_id_in_document=[]\n",
        "\n",
        "\n",
        "for root_id in range(len(list_xml_root)):\n",
        "  temp_list_argumentative_sections=[]\n",
        "  temp_list_arg_section_id_in_document=[]\n",
        "  new_temp_list_arg_section_id_in_document=[]\n",
        "  for child in list_xml_root[root_id]:\n",
        "    # The nodes tagged with 'edu' contains the text of the arg. section and an unique identifier in the document.\n",
        "    if child.tag=='edu':\n",
        "      temp_list_argumentative_sections.append(child.text)\n",
        "      temp_list_arg_section_id_in_document.append(child.get('id'))\n",
        "    # In the 'edge' nodes the original id of the sections (in src) are sobstitute with new ones (in trg).\n",
        "    if (child.tag=='edge'):\n",
        "      src=child.get('src')\n",
        "      trg=child.get('trg')\n",
        "      if src in temp_list_arg_section_id_in_document:\n",
        "        new_temp_list_arg_section_id_in_document.append(trg)\n",
        "  list_argumentative_sections.append(temp_list_argumentative_sections)\n",
        "  list_arg_section_id_in_document.append(new_temp_list_arg_section_id_in_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpwZU57o1mG8",
        "outputId": "ba6a11cf-ac18-4a19-da25-d03017d8bbc8"
      },
      "source": [
        "for i in range(len(list_argumentative_sections[0])):\n",
        "  print(\"section {} in document {}: {}\".format(list_arg_section_id_in_document[0][i],0,list_argumentative_sections[0][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "section a1 in document 0: Yes, it's annoying and cumbersome to separate your rubbish properly all the time.\n",
            "section a2 in document 0: Three different bin bags stink away in the kitchen and have to be sorted into different wheelie bins.\n",
            "section a3 in document 0: But still Germany produces way too much rubbish\n",
            "section a4 in document 0: and too many resources are lost when what actually should be separated and recycled is burnt.\n",
            "section a5 in document 0: We Berliners should take the chance and become pioneers in waste separation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX6NxN8X5WHQ"
      },
      "source": [
        "# **Extract sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOj56cDk5dvV"
      },
      "source": [
        "# All the argumentative sections are contained in a single sentence.\n",
        "\n",
        "# Get the list of all the sentences.\n",
        "\n",
        "sentence_list=[]\n",
        "\n",
        "for i in range(len(list_argumentative_sections)):\n",
        "  sentence=\"\"\n",
        "  for j in range(len(list_argumentative_sections[i])):\n",
        "\n",
        "    sentence+=list_argumentative_sections[i][j]\n",
        "\n",
        "    # An argumentative section which ends a sentence has a '.','?' or '!' as last character.\n",
        "    if sentence[-1] in ['.','?','!']:\n",
        "      sentence_list.append(sentence)\n",
        "      sentence=\"\"\n",
        "\n",
        "  if not sentence==\"\":  \n",
        "    sentence_list.append(sentence)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwuevinH6_mv",
        "outputId": "f50097ec-ffe8-4e15-da5f-fe8e8393c91c"
      },
      "source": [
        "sentence_list[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Yes, it's annoying and cumbersome to separate your rubbish properly all the time.\",\n",
              " 'Three different bin bags stink away in the kitchen and have to be sorted into different wheelie bins.',\n",
              " 'But still Germany produces way too much rubbishand too many resources are lost when what actually should be separated and recycled is burnt.',\n",
              " 'We Berliners should take the chance and become pioneers in waste separation!',\n",
              " 'One can hardly move in Friedrichshain or NeukÃ¶lln these days without permanently scanning the ground for dog dirt.',\n",
              " \"And when bad luck does strike and you step into one of the many 'land mines' you have to painstakingly scrape the remains off your soles.\",\n",
              " 'Higher fines are therefore the right measure against negligent, lazy or simply thoughtless dog owners.',\n",
              " \"Of course, first they'd actually need to be caught in the act by public order officers,but once they have to dig into their pockets, their laziness will sure vanish!\",\n",
              " 'Health insurance companies should not cover treatment in complementary medicineunless the promised effect and its medical benefit have been concretely proven.',\n",
              " 'Yet this very proof is lacking in most cases.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcO99LxgQchz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fe97e1-940f-434e-f066-417f6b01e18f"
      },
      "source": [
        "len(sentence_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "450"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__jT8z2IaT1R"
      },
      "source": [
        "# **List imorted n-gram words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6coemKvaZYx"
      },
      "source": [
        "# Get the list of non-repeated words in the n-gram imported.\n",
        "list_words=[]\n",
        "for ngram in claim_premise_indicators:\n",
        "  list_words+=ngram.split(\" \")\n",
        "\n",
        "list_words=list(set(list_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_JfbHE3IJPB",
        "outputId": "c004b8f6-f40a-4e50-d2cb-0f1b86d22cc2"
      },
      "source": [
        "list_words[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in',\n",
              " 'short',\n",
              " 'therefore',\n",
              " 'that',\n",
              " 'derived',\n",
              " 'reason',\n",
              " 'furthermore',\n",
              " 'due',\n",
              " 'as',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3faZf73ymhi"
      },
      "source": [
        "# **Customised Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIsQ9O1wyevB"
      },
      "source": [
        "#  Lemmatization is used.\n",
        "def build_tokenizer(text):\n",
        "\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Text tokenization.\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "\n",
        "  # Text normalization throgh lemmatizzation.\n",
        "\n",
        "  lemmas=[]\n",
        "  for word in tokens:\n",
        "    word_lemma=wordnet_lemmatizer.lemmatize(word)\n",
        "    if word_lemma in list_words:\n",
        "      lemmas.append(word_lemma)\n",
        "    else:\n",
        "      # characted \"|\" is not presente in the n-gram imported.\n",
        "      lemmas.append(\"|\")  \n",
        "\n",
        "  return lemmas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmHrfpQuXvN"
      },
      "source": [
        "# **Bag of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_xuJKjDqz8"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Trasformation of the passed corpus in the dataframe of the bag of ngram contained\n",
        "# in it.\n",
        "def bag_of_ngram(sentence_list: list, ngram: int)-> pd.DataFrame:\n",
        "  vectorizer=CountVectorizer(tokenizer=build_tokenizer,ngram_range=(1,ngram))\n",
        "  bag_ngram=vectorizer.fit_transform(sentence_list)\n",
        "\n",
        "  dataframe=pd.DataFrame(bag_ngram.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDNAN02UUsP"
      },
      "source": [
        "# **Bag of ngram export**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnlMmnA5bXGW"
      },
      "source": [
        "bag_ngram_dataframe=bag_of_ngram(sentence_list, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oIQrKdtlkNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c314b415-4968-4240-f370-eb7d54e50cbe"
      },
      "source": [
        "# Some of the n-gram in the imported list are not in the BOW just created. \n",
        "# Add them as empty columns.\n",
        "\n",
        "ngram_list=bag_ngram_dataframe.columns\n",
        "\n",
        "for ngram in claim_premise_indicators:\n",
        "  if not (ngram in ngram_list):\n",
        "    print(ngram)\n",
        "    bag_ngram_dataframe[ngram]=[0 for i in range(len(sentence_list))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "consequently\n",
            "conclude that\n",
            "demonstrates that\n",
            "implies\n",
            "in short\n",
            "in conclusion\n",
            "indicates that\n",
            "it follows that\n",
            "it should be clear that\n",
            "it should be clear\n",
            "suggests that\n",
            "to sum up\n",
            "assuming that\n",
            "as\n",
            "deduced\n",
            "derived from\n",
            "for the reason that\n",
            "given that\n",
            "in light of\n",
            "indicated by\n",
            "is supported by\n",
            "whereas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "r2hjU5FbdJOh",
        "outputId": "493dbbd6-68fd-4a51-fef4-6f777556f845"
      },
      "source": [
        "bag_ngram_dataframe[claim_premise_indicators].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accordingly</th>\n",
              "      <th>consequently</th>\n",
              "      <th>conclude that</th>\n",
              "      <th>clearly</th>\n",
              "      <th>demonstrates that</th>\n",
              "      <th>hence</th>\n",
              "      <th>implies</th>\n",
              "      <th>in short</th>\n",
              "      <th>in conclusion</th>\n",
              "      <th>indicates that</th>\n",
              "      <th>it follows that</th>\n",
              "      <th>it should be clear that</th>\n",
              "      <th>it should be clear</th>\n",
              "      <th>so</th>\n",
              "      <th>suggests that</th>\n",
              "      <th>therefore</th>\n",
              "      <th>thus</th>\n",
              "      <th>to sum up</th>\n",
              "      <th>assuming that</th>\n",
              "      <th>as</th>\n",
              "      <th>besides</th>\n",
              "      <th>because</th>\n",
              "      <th>deduced</th>\n",
              "      <th>derived from</th>\n",
              "      <th>due to</th>\n",
              "      <th>for</th>\n",
              "      <th>for example</th>\n",
              "      <th>for instance</th>\n",
              "      <th>for the reason that</th>\n",
              "      <th>furthermore</th>\n",
              "      <th>given that</th>\n",
              "      <th>in addition</th>\n",
              "      <th>in light of</th>\n",
              "      <th>in that</th>\n",
              "      <th>in view of</th>\n",
              "      <th>indicated by</th>\n",
              "      <th>is supported by</th>\n",
              "      <th>moreover</th>\n",
              "      <th>since</th>\n",
              "      <th>whereas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accordingly  consequently  conclude that  ...  moreover  since  whereas\n",
              "0            0             0              0  ...         0      0        0\n",
              "1            0             0              0  ...         0      0        0\n",
              "2            0             0              0  ...         0      0        0\n",
              "3            0             0              0  ...         0      0        0\n",
              "4            0             0              0  ...         0      0        0\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqzxXJTzcNlA"
      },
      "source": [
        "bag_ngram_dataframe[claim_premise_indicators].to_csv('microtext_BOW_appendix_words.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}