{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "external_retrieved_words_essay.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UV8lt9xVaK5",
        "outputId": "f4bd7883-b1a8-47b7-cdff-1402b1505acf"
      },
      "source": [
        "!pip install mendelai-brat-parser\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mendelai-brat-parser\n",
            "  Downloading mendelai_brat_parser-0.0.4-py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: mendelai-brat-parser\n",
            "Successfully installed mendelai-brat-parser-0.0.4\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD-3Hs9FVn7T"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import glob\n",
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "\n",
        "import regex as re\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQd31KC5YZMb"
      },
      "source": [
        "# Imported words and bigrams indicated in the appendix of the guidelines of \n",
        "# the Stab and Gurevych dataset as indicative of a premise or of a claim.\n",
        "# The n-gram which are absent in each one of the dataset considered have been\n",
        "# already removed from the list.\n",
        "\n",
        "claim_premise_indicators=['accordingly','consequently','conclude that','clearly','demonstrates that','hence','implies','in short','in conclusion','indicates that','it follows that',\n",
        "'it should be clear that','it should be clear','so','suggests that','therefore','thus','to sum up','assuming that','as',\n",
        "'besides','because','deduced','derived from','due to','for','for example','for instance','for the reason that','furthermore','given that','in addition','in light of','in that','in view of','indicated by','is supported by','moreover','since',\n",
        "'whereas']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-JScJrGVtFv"
      },
      "source": [
        "# **Import the dataset with the origninal corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KBcCdxcHVsmq",
        "outputId": "35b4732a-b8f4-46b3-94e4-a3cc4a358716"
      },
      "source": [
        "# Import the Stab and Gurevych dataset (2017 version).\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f0e73ce-a543-42a2-b7c4-f19728948059\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5f0e73ce-a543-42a2-b7c4-f19728948059\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ArgumentAnnotatedEssays-2.0.zip to ArgumentAnnotatedEssays-2.0.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KA9xIyVWDAh"
      },
      "source": [
        "# Function to extract zip file.\n",
        "# Takes in input the path to the zip file (path_zip) and the one to store the destination directory (path_destination).\n",
        "\n",
        "def extract_zip(path_zip,path_destination):\n",
        "  with zipfile.ZipFile(path_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path_destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE_5p8qap-Ib"
      },
      "source": [
        "# Extract zip file (two zip files one inside the other).\n",
        "\n",
        "extract_zip('ArgumentAnnotatedEssays-2.0.zip','ArgumentAnnotatedEssays-2.0')\n",
        "\n",
        "extract_zip('ArgumentAnnotatedEssays-2.0/ArgumentAnnotatedEssays-2.0/brat-project-final.zip','ArgumentAnnotatedEssays')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrsWTewaCnPf"
      },
      "source": [
        "# **Create the ann Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAvNRWA08Sg"
      },
      "source": [
        "# Extract the list of text files of the essays in the dataset.\n",
        "txt_files = sorted(glob.glob(\"ArgumentAnnotatedEssays/brat-project-final/essay*.txt\"))\n",
        "\n",
        "# Extract the list of text files of the essays in the dataset.\n",
        "ann_files = sorted(glob.glob(\"ArgumentAnnotatedEssays/brat-project-final/essay*.ann\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1yFSufe1iFC",
        "outputId": "7a08ef80-93f0-462c-dc53-7bca5e0b0ccb"
      },
      "source": [
        "# List of text files of the essays.\n",
        "txt_files[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArgumentAnnotatedEssays/brat-project-final/essay001.txt',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay002.txt',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay003.txt',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay004.txt',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay005.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yoE-YHW1ykn",
        "outputId": "9fcea49d-3189-4d93-d557-52cce92ba299"
      },
      "source": [
        "# List of the annotated files of the essays.\n",
        "ann_files[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArgumentAnnotatedEssays/brat-project-final/essay001.ann',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay002.ann',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay003.ann',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay004.ann',\n",
              " 'ArgumentAnnotatedEssays/brat-project-final/essay005.ann']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJtZWteKCka4"
      },
      "source": [
        "# Transfor the ann files into four dictionaries\n",
        "ann_disctionaries=[get_entities_relations_attributes_groups(file) for file in ann_files]\n",
        "\n",
        "# Transform the first dictionary (entities) obtained from each ann file into a dataset\n",
        "essay_ann_datasets = [pd.DataFrame.from_dict(entities, orient='index') for entities,_,_,_ in ann_disctionaries]\n",
        "\n",
        "# Add a coloumn into the dataset that identifies the document\n",
        "for i in range(len(essay_ann_datasets)):\n",
        "  essay_ann_datasets[i].insert(0,'doc_id',i)\n",
        "\n",
        "# Create a common dataset\n",
        "Essay_ann_dataset = pd.concat(essay_ann_datasets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShAWlc_o3RoB",
        "outputId": "e1d71948-c1d7-4027-f5da-2ea27a28263e"
      },
      "source": [
        "# Sobstitue the values in the \"span\" feature (substitute each tuple with its first inner tuple)\n",
        "for i in range(len(Essay_ann_dataset.index)):\n",
        "  Essay_ann_dataset['span'][i]=Essay_ann_dataset['span'][i][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RvtcIuEDGgY"
      },
      "source": [
        "Essay_ann_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMBkTfrkWZ5a"
      },
      "source": [
        "# **IOB and word list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1l-2YSD7_z"
      },
      "source": [
        "# List of texts of the essays\n",
        "files_text=[open(file).read() for file in txt_files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK-GkChLgEyl"
      },
      "source": [
        "# Check the presence of the character | (information used later).\n",
        "for essay in files_text:\n",
        "  if not (essay.find(\"|\") == -1):\n",
        "      print(\"One found!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Xv3S56l_HC"
      },
      "source": [
        "# Get the start and finish points of each argumentative section of each of the text.\n",
        "# Also get the type of each argumentative section.\n",
        "# This points are indicated in the \"span\" coloumn of the Dataframe of the annotation.\n",
        "\n",
        "sorted_span=[sorted(list(Essay_ann_dataset.loc[Essay_ann_dataset['doc_id'] == i,['span','type']].values), key=lambda element: (element[0][0]) ) for i in range(len(ann_files))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY761IVz8g2z"
      },
      "source": [
        "sorted_span[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB7ydWo78-x2"
      },
      "source": [
        "# Tranform the couples of points ((start,end) of each section) to a list (separator of different section).\n",
        "\n",
        "span_points=[]\n",
        "\n",
        "for i in range(len(sorted_span)):\n",
        "  list_points=[sep for sub in sorted_span[i] for sep in sub[0]]\n",
        "  # Insert starting point of the text\n",
        "  list_points.insert(0,0)\n",
        "  list_points.append(len(files_text[i]))\n",
        "  span_points.append(list_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFHv8OFV-MER"
      },
      "source": [
        "span_points[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV3tm2bS-p_t"
      },
      "source": [
        "# Separate the texts at the point indicated for the different sections\n",
        "\n",
        "split_text=[]\n",
        "\n",
        "for z in range(len(files_text)):\n",
        "  split_text.append([files_text[z][i: j] for i, j in zip(span_points[z], span_points[z][1:])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyylYuLL_SEb"
      },
      "source": [
        "split_text[0][0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVRUlXHAwa3"
      },
      "source": [
        "# Get the list of the words of each essay and the corresponding labels \n",
        "# ( argumentative section, premise or claim indicated through IOB).\n",
        "\n",
        "Y_IOB=[]\n",
        "X_essay_word_list=[]\n",
        "\n",
        "for i in range(len(split_text)):\n",
        "  \n",
        "  IOB=[]\n",
        "  essay_word_list=[]\n",
        "\n",
        "  # next_type is the next position still not considered in the list of argumentative sections of the essay examined.\n",
        "  next_type=0\n",
        "\n",
        "  # Remove title from essay considered (splitted_text)\n",
        "  no_title=[re.sub(r\".*\\n\\n\",\"\",text) for text in split_text[i]]\n",
        "\n",
        "  no_title=[re.sub(r\"\\n\",\" | \",text) for text in no_title]\n",
        "\n",
        "  # Set boolean value next_token_is_argumentative to false.\n",
        "  # (first section of each essay is never argumentative, it's the one containing the title).\n",
        "  next_token_is_argumentative=False\n",
        "\n",
        "  for section in no_title:\n",
        "\n",
        "    # Divide the text into token.\n",
        "    seq=nltk.word_tokenize(section.lower())\n",
        "\n",
        "    essay_word_list+=seq\n",
        "\n",
        "    if next_token_is_argumentative:\n",
        "      if sorted_span[i][next_type][1]=='Premise':\n",
        "        IOB+=['B-P']\n",
        "        for token in range(len(seq)-1):\n",
        "          IOB+=['I-P']\n",
        "      else:\n",
        "        IOB+=['B-C']\n",
        "        for token in range(len(seq)-1):\n",
        "          IOB+=['I-C']\n",
        "      next_type += 1     \n",
        "    else:\n",
        "      for token in range(len(seq)):\n",
        "        IOB+=['O']\n",
        "\n",
        "    # An argumentative section is followed by a non-argumentative section and vice-versa    \n",
        "    next_token_is_argumentative=not next_token_is_argumentative\n",
        "\n",
        "  Y_IOB.append(IOB)\n",
        "  X_essay_word_list.append(essay_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK9ZdPIJC2VA"
      },
      "source": [
        "for i in range(80,120):\n",
        "  print(X_essay_word_list[0][i]+\" - \"+Y_IOB[0][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F89T0f2wX-Zp"
      },
      "source": [
        "# **Find out sentences which contain argumentative section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPcb2rgQUmzn"
      },
      "source": [
        "# Get the list of all the sentences and the fact that they contain \n",
        "# an argumentative section, a premise or a claim or not.\n",
        "\n",
        "sentence_list=[]\n",
        "sentences_argumentative_map=[]\n",
        "sentences_claim_presence_map=[]\n",
        "sentences_premise_presence_map=[]  \n",
        "\n",
        "for essay_id in range(len(X_essay_word_list)):\n",
        "  sentence=\"\"\n",
        "  argumentative=False\n",
        "  claim=False\n",
        "  premise=False\n",
        "\n",
        "  for word_id in range(len(X_essay_word_list[essay_id])):\n",
        "    \n",
        "    word=X_essay_word_list[essay_id][word_id]\n",
        "    bio_of_word=Y_IOB[essay_id][word_id]\n",
        "\n",
        "    if not (bio_of_word=='O'):\n",
        "      argumentative=True\n",
        "      if (bio_of_word=='I-C') or (bio_of_word=='B-C'):\n",
        "        claim=True\n",
        "      else:\n",
        "        premise=True  \n",
        "\n",
        "    # Add to the sentence every word that is not a simple \\n or the end of the sentence (\".\").\n",
        "    if not ( word==\"|\" or word==\".\" ):\n",
        "      sentence+=word+\" \"\n",
        "          \n",
        "    # '.', '?' or '!' is the end of the sentence.       \n",
        "    if word in [\".\",\"?\",\"!\"] and ( not sentence==\"\"):\n",
        "      sentence_list.append(sentence)\n",
        "\n",
        "      # Add the indicator of the presence of argumentative sections to the correspondent lists.\n",
        "      sentences_argumentative_map.append(argumentative)\n",
        "      sentences_claim_presence_map.append(claim)\n",
        "      sentences_premise_presence_map.append(premise)\n",
        "      \n",
        "      argumentative=False\n",
        "      claim=False\n",
        "      premise=False\n",
        "      sentence=\"\"\n",
        "\n",
        "  if not sentence==\"\":  \n",
        "    sentence_list.append(sentence)  \n",
        "\n",
        "    # Add the indicator of the presence of argumentative sections to the correspondent lists.\n",
        "    sentences_argumentative_map.append(argumentative)\n",
        "    sentences_claim_presence_map.append(claim)\n",
        "    sentences_premise_presence_map.append(premise)\n",
        "      \n",
        "    argumentative=False\n",
        "    claim=False\n",
        "    premise=False\n",
        "    sentence=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__jT8z2IaT1R"
      },
      "source": [
        "# **List imported n-gram words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6coemKvaZYx"
      },
      "source": [
        "# Get the list of non-repeated words in the n-gram imported.\n",
        "list_words=[]\n",
        "for ngram in claim_premise_indicators:\n",
        "  list_words+=ngram.split(\" \")\n",
        "\n",
        "list_words=list(set(list_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_JfbHE3IJPB"
      },
      "source": [
        "list_words[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3faZf73ymhi"
      },
      "source": [
        "# **Customised Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIsQ9O1wyevB"
      },
      "source": [
        "#  Lemmatization is used.\n",
        "def build_tokenizer(text):\n",
        "\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Text tokenization.\n",
        "  tokens=nltk.word_tokenize(text)\n",
        "\n",
        "  # Text normalization throgh lemmatizzation.\n",
        "\n",
        "  lemmas=[]\n",
        "  for word in tokens:\n",
        "    word_lemma=wordnet_lemmatizer.lemmatize(word)\n",
        "    if word_lemma in list_words:\n",
        "      lemmas.append(word_lemma)\n",
        "    else:\n",
        "      # characted \"|\" is not presente in the original corpus.\n",
        "      lemmas.append(\"|\")  \n",
        "\n",
        "  return lemmas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmHrfpQuXvN"
      },
      "source": [
        "# **Bag of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx_xuJKjDqz8"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Trasformation of the passed corpus in the dataframe of the bag of ngram contained\n",
        "# in it.\n",
        "def bag_of_ngram(sentence_list: list, ngram: int)-> pd.DataFrame:\n",
        "  vectorizer=CountVectorizer(tokenizer=build_tokenizer,ngram_range=(1,ngram))\n",
        "  bag_ngram=vectorizer.fit_transform(sentence_list)\n",
        "\n",
        "  dataframe=pd.DataFrame(bag_ngram.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDNAN02UUsP"
      },
      "source": [
        "# **Bag of ngram export**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnlMmnA5bXGW"
      },
      "source": [
        "bag_ngram_dataframe=bag_of_ngram(sentence_list, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oIQrKdtlkNB"
      },
      "source": [
        "# Some of the n-gram in the imported list are not in the BOW just created. \n",
        "# Add them as empty columns.\n",
        "\n",
        "ngram_list=bag_ngram_dataframe.columns\n",
        "\n",
        "for ngram in claim_premise_indicators:\n",
        "  if not (ngram in ngram_list):\n",
        "    print(ngram)\n",
        "    bag_ngram_dataframe[ngram]=[0 for i in range(len(sentence_list))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2hjU5FbdJOh"
      },
      "source": [
        "bag_ngram_dataframe[claim_premise_indicators].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqzxXJTzcNlA"
      },
      "source": [
        "bag_ngram_dataframe[claim_premise_indicators].to_csv('essay_BOW_appendix_words.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}